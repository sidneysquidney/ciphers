{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram frequency analysis\n",
    "\n",
    "We've already looked at the frequency of letters in English. But now we'll extend it to general n-length consecutive characters to characterise how likely some text is to be English. We can think of this as finding the probability that some given bit of text appearing in the english language. This probability is the product of all the individual parts of the text occurring multiplied together. A simple example is\n",
    "$$\n",
    "P(`word') = P('wo') \\times P('or') \\times P('rd')\n",
    "$$\n",
    "if we are thinking in terms of bigrams. Even if we have a long message we can still calculate the probability of it occurring by breaking it down into probabilities of all the bigrams contained in the text (or trigrams etc). What we need to know are these bigram probabilities. Basically how frequently does 'wo' appear? This \n",
    "\n",
    "We find the counts of all the n-grams in the text: `count` = $c$.\n",
    "Eg for mono-grams we might have:\n",
    "```python\n",
    "characters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "count = [112,54,32,210,78,...]\n",
    "```\n",
    "or for bi-grams:\n",
    "```python\n",
    "characters = ['aa','ab','ac','ad', ...]\n",
    "count = [12,2,5,24,...]\n",
    "```\n",
    "The *likelihood* of the whole text is then the product of all the probabilities of all these n-grams. Let's say we have $N$ n-grams which we call $NG$ in our text (so just the number of characters for mono-grams), so our notation is that the $n$-th n-gram is labelled $\\text{NG}_n$. The likelihood $L$ is then\n",
    "$$\n",
    "L = \\prod_{n=1}^N p(\\text{NG}_n)\n",
    "$$\n",
    "$$\n",
    "\\log(L) = \\sum_{n=1}^N \\log(p(\\text{NG}_n))\n",
    "$$\n",
    "So we could loop over all the ngrams we have have and add up all of them like above. \n",
    "eg. if our text is: 'thisissometextthanks' we could loop over the list of n-grams:\n",
    "```python\n",
    "['th','hi','is','ss','so','om','me','et','te','ex','xt','tt','th','ha','an','nk','ks']\n",
    "```\n",
    "This would work and wouldn't require finding the counts first but if we do have the counts, which would be:\n",
    "```python\n",
    "bigrams = ['th','hi','is','ss','so','om','me','et','te','ex','xt','tt','ha','an','nk','ks']\n",
    "count = [2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "```\n",
    "then it can be calculated as\n",
    "$$\n",
    "\\log(L) = \\sum_{m=1}^{\\text{len(count)}} c_m p(\\text{NG}_m)\n",
    "$$\n",
    "which may be easier but it's up to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the two formulas for $\\log(L)$ give two ways to compute the log-likelihood and it's up to you which you choose. Hopefully it will make some sense\n",
    "\n",
    "With the log-likelihood we can then use a hill climb or genetic algorithm to improve our guesses\n",
    "\n",
    "<b>Note:</b> How can you check it's working before all the code is done? It is good to get into the habit of thinking how you can test individual functions in your code. Try to calculate the log-likelihood for the *original* bee movie text and compare to the encrypted, the former should have a higher value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count - array of the counts for all possible ngrams\n",
    "# probability - probability of their occurance using the data from the file\n",
    "def log_likelihood(count, prob):\n",
    "    logL = ...\n",
    "    return logL\n",
    "\n",
    "# ngrams - list of all the ngrams in the text \n",
    "# probability - probability of each ngram occurance using the data from the file\n",
    "def log_likelihood(ngrams, prob):\n",
    "    logL = ...\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, these probabilities are just the frequency at which they occur in the english language, in other words the frequencies calculated from the mono/bi-gram files.\n",
    "\n",
    "Use your guess from the character frequencies as a first guess then use the bigrams to try and improve upon it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill-climb\n",
    "\n",
    "for more details see [Hill-climbing](https://en.wikipedia.org/wiki/Hill_climbing).\n",
    "\n",
    "So to improve our guess of what the key is we:\n",
    "- Start from an initial guess, this can be random or you can start from the simple character frequency analysis guess\n",
    "- calculate the likelihood of this guess\n",
    "- Then we find all the neighbours of our guess, this means all of the keys which have two characters flipped. \n",
    "- Then we calculate the likelihood for all of these neighbours\n",
    "- see which improves our liklihood the most (if any) and change our guess to that\n",
    "- repeat the previous steps either until it stops improving or until it reaches like a 1000 iterations or whatever depending on how slow it is\n",
    "\n",
    "remember, to calculate the likelihood you'll have to take the key and decrypt the message and then find the counts of the bigrams and pass that to the likelihood function.\n",
    "\n",
    "I hope that makes sense and you can get it to work, not sure how slow it will be though.\n",
    "\n",
    "If you have time and are interested you can also look up the [genetic algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm) and have a go with the likelihood function. It may actually be less work than the hill-climb since you just generate random keys, combine them randomly and select the best ones etc rather than having to find all the neighbours .\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee44c854a9b80cad61487e373c308766c9a648a8ec0d2e75d2343bb1b568fdcb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
